{"instruction": "Create a short, concise summary of the paper based on its abstract.\n\nFew-shot learning (FSL) is one of the key future steps in machine learning and raises a lot of attention. In this paper, we focus on the FSL problem of dialogue understanding, which contains two closely related tasks: intent detection and slot filling. Dialogue understanding has been proven to benefit a lot from jointly learning the two sub-tasks. However, such joint learning becomes challenging in the few-shot scenarios: on the one hand, the sparsity of samples greatly magnifies the difficulty of modeling the connection between the two tasks; on the other hand, how to jointly learn multiple tasks in the few-shot setting is still less investigated. In response to this, we introduce FewJoint, the first FSL benchmark for joint dialogue understanding. FewJoint provides a new corpus with 59 different dialogue domains from real industrial API and a code platform to ease FSL experiment set-up, which are expected to advance the research of this field. Further, we find that insufficient performance of the few-shot setting often leads to noisy sharing between two sub-task and disturbs joint learning. To tackle this, we guide slot with explicit intent information and propose a novel trust gating mechanism that blocks low-confidence intent information to ensure high quality sharing. Besides, we introduce a Reptile-based meta-learning strategy to achieve better generalization in unseen few-shot domains. In the experiments, the proposed method brings significant improvements on two datasets and achieve new state-of-the-art performance.", "output": "The paper focuses on the few-shot learning (FSL) problem of dialogue understanding, which involves two tasks: intent detection and slot filling. The authors introduce FewJoint, a new benchmark for FSL in dialogue understanding, which includes a corpus of 59 different dialogue domains from real industrial API and a code platform for experiment set-up. They find that insufficient performance in the few-shot setting can lead to noisy sharing between the two sub-tasks, which disturbs joint learning. To address this, they propose a novel trust gating mechanism that blocks low-confidence intent information and a Reptile-based meta-learning strategy for better generalization in unseen few-shot domains. The proposed method achieves significant improvements on two datasets and sets a new state-of-the-art performance.", "generator": "./save_merge_models/instruct_math/instruct/average_merging", "dataset": "selfinstruct"}
