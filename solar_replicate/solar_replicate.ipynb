{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from copy import deepcopy\n",
    "\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 加载model \n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "origin_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"/home/huyanwei/.cache/huggingface/hub\",\n",
    "    # device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# 获取原始模型的配置\n",
    "config = origin_model.config\n",
    "\n",
    "# 修改config \n",
    "config._name_or_path = \"Mistral-7B*2-48layers\"\n",
    "\n",
    "# merge后模型存储位置 \n",
    "save_dir = f\"./save_model/{config._name_or_path}\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 使用深拷贝复制模型\n",
    "copy_model = deepcopy(origin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始模型的 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 将 tokenizer 保存到新模型的目录\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存放merge后的层 \n",
    "new_layers = []\n",
    "\n",
    "# C0~C15 = A0~A15\n",
    "new_layers.extend(deepcopy(origin_model.model.layers[:16]))\n",
    "\n",
    "# C16~C23 = A16~A23 + B0～B7，对应层加权平均\n",
    "for i in range(16, 24):\n",
    "    new_layer = deepcopy(origin_model.model.layers[i])\n",
    "    for param_name, param_tensor in new_layer.named_parameters():\n",
    "        param_tensor.data.copy_((param_tensor.data + deepcopy(copy_model.model.layers[i - 16].state_dict()[param_name].data)) * 0.5)\n",
    "    new_layers.append(new_layer)\n",
    "\n",
    "# C24~C31 = A24~A31 + B8～B15，对应层加权平均\n",
    "for i in range(24, 32):\n",
    "    new_layer = deepcopy(origin_model.model.layers[i])\n",
    "    for param_name, param_tensor in new_layer.named_parameters():\n",
    "        param_tensor.data.copy_((param_tensor.data + deepcopy(copy_model.model.layers[i - 16].state_dict()[param_name].data)) * 0.5)\n",
    "    new_layers.append(new_layer)\n",
    "\n",
    "# C32~C47 = B16～B31\n",
    "new_layers.extend(deepcopy(copy_model.model.layers[16:]))\n",
    "\n",
    "# 将新层列表分配给原始模型的层\n",
    "if isinstance(origin_model, torch.nn.DataParallel):\n",
    "    origin_model.module.model.layers = torch.nn.ModuleList(new_layers)\n",
    "else:\n",
    "    origin_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "\n",
    "config.num_hidden_layers = len(new_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 方法\n",
    "def merge_models(origin_model, copy_model, n, s, m):\n",
    "    \"\"\"\n",
    "        n  # Original number of layers in the origin model\n",
    "        s  # Total number of layers desired in the new model\n",
    "        m  # Number of layers to be merged\n",
    "    \"\"\"\n",
    "\n",
    "    # 存放merge后的层\n",
    "    new_layers = []\n",
    "    \n",
    "    # Step 1: Copy the initial layers from the origin model as is\n",
    "    # C0~C(n-m-1) = A0~A(n-m-1)\n",
    "    new_layers.extend(deepcopy(origin_model.model.layers[:n-m]))\n",
    "    \n",
    "    # Step 2: Merge the overlapping layers by averaging\n",
    "    # C(n-m)~C(n-1) = A(n-m)~A(n-1) + B0～B(m-1)，对应层加权平均\n",
    "    for i in range(n-m, n):\n",
    "        new_layer = deepcopy(origin_model.model.layers[i])\n",
    "        for param_name, param_tensor in new_layer.named_parameters():\n",
    "            param_tensor.data.copy_((param_tensor.data + deepcopy(copy_model.model.layers[i - (n-m)].state_dict()[param_name].data)) * 0.5)\n",
    "        new_layers.append(new_layer)\n",
    "    \n",
    "    # Step 3: Add the remaining layers from the copy model\n",
    "    # Cn~C(s-1) = Bm～B(s-n+m-1)\n",
    "    new_layers.extend(deepcopy(copy_model.model.layers[m:s-n+m]))\n",
    "\n",
    "    # Replace the layers in the original model with the new merged layers\n",
    "    origin_model.model.layers = torch.nn.ModuleList(new_layers)\n",
    "    \n",
    "    return origin_model\n",
    "\n",
    "# Define the parameters\n",
    "n = 32  # Original number of layers in the origin model\n",
    "s = 48  # Total number of layers desired in the new model\n",
    "m = 8   # Number of layers to be merged\n",
    "\n",
    "\n",
    "# Merge the models\n",
    "merged_model = merge_models(origin_model, copy_model, n, s, m)\n",
    "\n",
    "# Save the merged model\n",
    "if isinstance(origin_model, torch.nn.DataParallel):\n",
    "    origin_model.module.save_pretrained(save_dir)\n",
    "else:\n",
    "    origin_model.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"新模型已保存到：{save_dir}; 新模型隐藏层个数：{len(origin_model.model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代模型的所有子模块和层，并打印\n",
    "for name, module in origin_model.named_modules():\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一个特定的层来检查\n",
    "layer_index = 5  # 例如，我们选择第6层，因为索引是从0开始的\n",
    "\n",
    "# 打印特定层的参数\n",
    "def print_layer_parameters(model, layer_index):\n",
    "    layer = model.model.layers[layer_index]\n",
    "    for param_tensor in layer.state_dict():\n",
    "        print(param_tensor, \"\\t\", layer.state_dict()[param_tensor].size())\n",
    "        print(layer.state_dict()[param_tensor])\n",
    "\n",
    "print(\"特定层的参数权重：\")\n",
    "print_layer_parameters(origin_model, layer_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "origin_model.to(device)\n",
    "\n",
    "# 准备一些输入文本作为提示\n",
    "input_text = \"这是一个例子文本\"  # 可以根据需要更改\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 使用 generate 方法进行文本生成\n",
    "# 您可以调整 generate 方法的参数来改变生成的结果，如max_length、num_beams等\n",
    "generated_ids = origin_model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "\n",
    "# 将生成的 token ID 解码为文本\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"输入文本:\", input_text)\n",
    "print(\"生成文本:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试 2\n",
    "\n",
    "text = \"Hi, my name is \"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = origin_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用MMLU 进行评测 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
